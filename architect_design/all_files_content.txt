Project Structure:

multivendor-catalog/
    .gitattributes
    docker-compose.yml
    requirements.txt
    backend/
        app/
            Dockerfile
            main.py
            api/
                routes.py
            core/
                config.py
                database.py
                kafka_producer.py
                logger.py
                postgres_database.py
            logs/
        flink_jobs/
            debezium_sync.py
            flink_job.py
            debezium_connectors/
                debezium-postgres-connector.json
                debezium-sqlite-connector.json
    databases/
        postgresql/
            init_postgres_schema.sql
        sqlite/
            init_schema.sql
            products.db
            seed_data.sql
    flink/
        Dockerfile
    logs/
    mobile_app/
        assets/
            images/
                image.png
        lib/
            main.dart
            screens/
                product_list_screen.dart
            services/
                logger_service.dart

==================================================

File Contents:

.gitattributes :

# Auto detect text files and perform LF normalization
* text=auto


==================================================

docker-compose.yml :

services:
  fastapi:
    container_name: fastapi_service
    build:
      context: .
      dockerfile: backend/app/Dockerfile
    volumes:
      - ./backend/app:/app
      - ./databases/sqlite:/databases/sqlite
      - ./logs:/app/logs
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=sqlite:///databases/sqlite/products.db
      - KAFKA_BROKER_URL=kafka:9092
      - POSTGRES_HOST=postgres
      - SQLITE_DB_PATH=/databases/sqlite/products.db
    depends_on:
      - kafka
      - postgres
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka_service
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_MESSAGE_MAX_BYTES: '10485760'  # 10 MB
      KAFKA_MAX_REQUEST_SIZE: '10485760'  # 10 MB
      KAFKA_RECEIVE_BUFFER_BYTES: '10485760'  # 10 MB
      KAFKA_SEND_BUFFER_BYTES: '10485760'  # 10 MB
    depends_on:
      - zookeeper
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --list --bootstrap-server localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3

  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper_service
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 30s
      timeout: 10s
      retries: 3

  postgres:
    image: postgres:13
    container_name: postgres_service
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: product_catalog
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./databases/postgresql/init_postgres_schema.sql:/docker-entrypoint-initdb.d/init_postgres_schema.sql
    ports:
      - "5432:5432"
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 3

  debezium:
    image: debezium/connect:latest
    container_name: debezium_service
    ports:
      - "8083:8083"
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
      - CONNECT_BOOTSTRAP_SERVERS=kafka:9092
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false
    depends_on:
      - kafka
      - postgres
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  flink-jobmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink_jobmanager
    ports:
      - "8081:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    volumes:
      - ./backend/flink_jobs:/opt/flink/jobs
      - ./databases/sqlite:/databases/sqlite
      - ./logs:/opt/flink/logs
    networks:
      - app_network
    depends_on:
      - kafka
    command: jobmanager
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 30s
      timeout: 10s
      retries: 3

  flink-taskmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink_taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    volumes:
      - ./backend/flink_jobs:/opt/flink/jobs
      - ./databases/sqlite:/databases/sqlite
      - ./logs:/opt/flink/logs
    networks:
      - app_network
    depends_on:
      - flink-jobmanager
    command: taskmanager

  flink-job-submitter:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink_job_submitter
    volumes:
      - ./backend/flink_jobs:/opt/flink/jobs
    networks:
      - app_network
    depends_on:
      - flink-jobmanager
      - flink-taskmanager
    command: >
      /bin/bash -c "
      echo 'Waiting for Flink JobManager to be ready...' &&
      sleep 30 &&
      echo 'Submitting Flink job...' &&
      flink run -d -py /opt/flink/jobs/flink_job.py
      "

volumes:
  postgres_data:

networks:
  app_network:
    driver: bridge


==================================================

requirements.txt :

Binary file, content not displayed.


==================================================

backend/app/Dockerfile :

FROM python:3.9-slim

WORKDIR /app

# Sistem bağımlılıklarını yükleyin (sqlite3 dahil)
RUN apt-get update && apt-get install -y \
    gcc \
    libpq-dev \
    sqlite3 \
    && rm -rf /var/lib/apt/lists/*

# Copy the requirements file into the container
COPY ../../requirements.txt .

# Install the Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code
COPY ../../backend/app /app

# Ensure the module imports work correctly
ENV PYTHONPATH=/app

# Expose the port FastAPI will run on
EXPOSE 8000

# Command to run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]


==================================================

backend/app/main.py :

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from api.routes import router
from core.config import settings
from core.logger import setup_logger, log_step, daily_log_maintenance
import os
import sqlite3
import asyncio

logger = setup_logger('main')

@asynccontextmanager
async def lifespan(app: FastAPI):
    log_step(logger, 1, "Starting up FastAPI application")
    
    # Log bakımı için bir background task başlat
    asyncio.create_task(periodic_log_maintenance())

    # Startup event
    db_path = '/databases/sqlite/products.db'
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    log_step(logger, 2, "Initializing SQLite schema")
    with open('/databases/sqlite/init_schema.sql') as f:
        conn.executescript(f.read())

    log_step(logger, 3, "Checking if vendors table is empty")
    cursor.execute("SELECT COUNT(*) FROM vendors;")
    result = cursor.fetchone()
    vendor_count = result[0] if result else 0

    if vendor_count == 0:
        log_step(logger, 4, "Seeding vendors data")
        seed_file_path = '/databases/sqlite/seed_data.sql'
        if os.path.exists(seed_file_path):
            with open(seed_file_path) as f:
                conn.executescript(f.read())

    conn.close()

    log_step(logger, 5, "Startup complete")
    yield
    log_step(logger, 6, "Shutting down FastAPI application")

app = FastAPI(title=settings.PROJECT_NAME, lifespan=lifespan)

app.include_router(router)

origins = ["*"]
    
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def periodic_log_maintenance():
    while True:
        daily_log_maintenance()
        await asyncio.sleep(24 * 60 * 60)  # 24 saat bekle

log_step(logger, 7, "FastAPI application configured and ready")

==================================================

backend/app/api/routes.py :

from fastapi import APIRouter, Depends, HTTPException
from core.database import get_db_connection
from core.kafka_producer import send_data_to_kafka
from core.logger import setup_logger, log_step

router = APIRouter()
logger = setup_logger('api_routes')

@router.get("/products")
def get_products(vendor_id: int = None, conn=Depends(get_db_connection)):
    log_step(logger, 1, f"Received request for products. Vendor ID: {vendor_id}")
    cursor = conn.cursor()
    if vendor_id:
        log_step(logger, 2, f"Fetching products for vendor ID: {vendor_id}")
        cursor.execute("""
            SELECT p.*
            FROM products p
            JOIN products_vendors pv ON p.id = pv.product_id
            WHERE pv.vendor_id = ?
        """, (vendor_id,))
    else:
        log_step(logger, 2, "Fetching all products")
        cursor.execute("SELECT * FROM products")
    
    products = cursor.fetchall()
    log_step(logger, 3, f"Fetched {len(products)} products")

    if not products:
        log_step(logger, 4, "No products found. Checking vendors table.")
        cursor.execute("SELECT COUNT(*) FROM vendors")
        vendor_count = cursor.fetchone()[0]
        log_step(logger, 5, f"Found {vendor_count} vendors in the database")

        if vendor_count == 0:
            log_step(logger, 6, "No vendors found. Please check if the database is properly seeded.")
            raise HTTPException(status_code=404, detail="No vendors found in the database")

        log_step(logger, 7, "Initiating data fetch process")
        if vendor_id:
            log_step(logger, 8, f"Sending Kafka message for vendor ID: {vendor_id}")
            send_data_to_kafka({'vendor_id': vendor_id})
        else:
            log_step(logger, 8, "Sending Kafka messages for all vendors")
            cursor.execute("SELECT id FROM vendors")
            vendor_ids = cursor.fetchall()
            for vendor in vendor_ids:
                log_step(logger, 9, f"Sending Kafka message for vendor ID: {vendor['id']}")
                send_data_to_kafka({'vendor_id': vendor['id']})
        
        log_step(logger, 10, "Raising HTTPException with status code 202")
        raise HTTPException(status_code=202, detail="Data is being fetched, please try again shortly.")
    
    log_step(logger, 11, f"Returning {len(products)} products")
    return {"products": [dict(ix) for ix in products]}

@router.post("/request-vendor-data")
def request_vendor_data(vendor_id: int, conn=Depends(get_db_connection)):
    log_step(logger, 1, f"Received request to fetch data for vendor ID: {vendor_id}")
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM vendors WHERE id = ?", (vendor_id,))
    vendor = cursor.fetchone()
    if not vendor:
        log_step(logger, 2, f"Invalid vendor ID: {vendor_id}")
        raise HTTPException(status_code=400, detail="Invalid vendor_id")
    log_step(logger, 3, f"Sending Kafka message for vendor ID: {vendor_id}")
    send_data_to_kafka({'vendor_id': vendor_id})
    log_step(logger, 4, f"Data fetch request for vendor {vendor_id} has been sent")
    return {"message": f"Data fetch request for vendor {vendor_id} has been sent."}

==================================================

backend/app/core/config.py :

import os
from core.logger import setup_logger, log_step

logger = setup_logger('config')

class Settings:
    PROJECT_NAME: str = "Unified Vendor Catalog"
    SQLALCHEMY_DATABASE_URL: str = os.getenv("DATABASE_URL", "sqlite:///../../databases/sqlite/products.db")
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")

settings = Settings()

log_step(logger, 1, f"Configuration loaded: PROJECT_NAME={settings.PROJECT_NAME}, DATABASE_URL={settings.SQLALCHEMY_DATABASE_URL}, LOG_LEVEL={settings.LOG_LEVEL}")

==================================================

backend/app/core/database.py :

import sqlite3
from typing import Generator
from core.logger import setup_logger, log_step

logger = setup_logger('database')

def get_db_connection() -> Generator:
    db_path = '/databases/sqlite/products.db'
    log_step(logger, 1, f"Attempting to connect to database: {db_path}")
    try:
        conn = sqlite3.connect(db_path, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        log_step(logger, 2, "Database connection established successfully")
        try:
            yield conn
        finally:
            log_step(logger, 3, "Closing database connection")
            conn.close()
            log_step(logger, 4, "Database connection closed")
    except sqlite3.Error as e:
        log_step(logger, 5, f"Error connecting to database: {str(e)}")
        raise

==================================================

backend/app/core/kafka_producer.py :

import sys
from kafka import KafkaProducer
import json
import os
import time
from core.logger import setup_logger, log_step

# Kafka configuration
KAFKA_BROKER_URL = os.getenv('KAFKA_BROKER_URL', 'kafka:9092')
TOPIC_NAME = 'vendor_requests'
MAX_MESSAGE_SIZE = 5 * 1024 * 1024  # 5 MB

logger = setup_logger('kafka_producer')
producer = None

def get_kafka_producer():
    global producer
    if producer is None:
        for attempt in range(5):  # Retry up to 5 times
            try:
                log_step(logger, 1, f"Attempting to connect to Kafka broker (Attempt {attempt + 1})")
                producer = KafkaProducer(
                    bootstrap_servers=[KAFKA_BROKER_URL],
                    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                    max_request_size=MAX_MESSAGE_SIZE,
                    buffer_memory=33554432,  # 32MB
                    batch_size=16384,
                    linger_ms=100,
                    max_block_ms=5000,
                    request_timeout_ms=30000,
                    api_version_auto_timeout_ms=5000
                )
                log_step(logger, 2, f"Successfully connected to Kafka broker on attempt {attempt + 1}")
                break
            except Exception as e:
                log_step(logger, 3, f"Failed to connect to Kafka broker on attempt {attempt + 1}. Error: {str(e)}")
                time.sleep(5)
        else:
            log_step(logger, 4, "Could not establish connection to Kafka broker after multiple attempts")
            raise Exception("Could not establish connection to Kafka broker after multiple attempts.")
    return producer

def get_size(obj, seen=None):
    """Recursively calculate size of objects"""
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    seen.add(obj_id)
    if isinstance(obj, dict):
        size += sum([get_size(v, seen) for v in obj.values()])
        size += sum([get_size(k, seen) for k in obj.keys()])
    elif hasattr(obj, '__dict__'):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):
        size += sum([get_size(i, seen) for i in obj])
    return size

def send_data_to_kafka(data):
    try:
        log_step(logger, 1, f"Preparing to send data to Kafka topic: {TOPIC_NAME}")
        log_step(logger, 2, f"Data size: {get_size(data)} bytes")
        
        kafka_producer = get_kafka_producer()
        serialized_data = json.dumps(data).encode('utf-8')
        
        log_step(logger, 3, f"Serialized data size: {len(serialized_data)} bytes")
        
        if len(serialized_data) > MAX_MESSAGE_SIZE:
            log_step(logger, 4, f"WARNING: Data size ({len(serialized_data)} bytes) exceeds maximum message size ({MAX_MESSAGE_SIZE} bytes)")
            log_step(logger, 5, f"Data preview: {str(data)[:500]}...")  # Log first 500 characters of the data
            
            # Implement message splitting logic here if needed
            # For now, we'll just log a warning and try to send the data as is
        
        future = kafka_producer.send(TOPIC_NAME, value=data)
        result = future.get(timeout=60)
        log_step(logger, 6, f"Data sent to Kafka topic: {TOPIC_NAME}, partition: {result.partition}, offset: {result.offset}")
    except Exception as e:
        log_step(logger, 7, f"Failed to send data to Kafka: {str(e)}")
        log_step(logger, 8, f"Error details: {str(e)}")
        log_step(logger, 9, f"Data that failed to send: {str(data)[:1000]}...")  # Log first 1000 characters of the data


==================================================

backend/app/core/logger.py :

import logging
import os
from datetime import datetime
from logging.handlers import RotatingFileHandler

def setup_logger(name):
    log_dir = '/app/logs'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    today = datetime.now().strftime('%Y-%m-%d')
    log_file = os.path.join(log_dir, f'app_{today}.log')
    
    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # RotatingFileHandler kullanarak log dosyasının boyutunu sınırlayalım
    file_handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    return logger

def log_step(logger, step, message):
    logger.info(f"STEP {step}: {message}")

def archive_old_logs(log_dir, days_to_keep=30):
    import shutil
    from datetime import timedelta

    today = datetime.now()
    for filename in os.listdir(log_dir):
        if filename.startswith('app_') and filename.endswith('.log'):
            file_date_str = filename[4:14]  # Extract date from filename
            file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
            if (today - file_date).days > days_to_keep:
                src = os.path.join(log_dir, filename)
                dst = os.path.join(log_dir, 'archive', filename)
                os.makedirs(os.path.dirname(dst), exist_ok=True)
                shutil.move(src, dst)

# Her gün bir kez çalıştırılacak bir fonksiyon
def daily_log_maintenance():
    log_dir = '/app/logs'
    archive_old_logs(log_dir)

==================================================

backend/app/core/postgres_database.py :

import os
import psycopg2
from psycopg2.extras import RealDictCursor
from core.logger import setup_logger, log_step

logger = setup_logger('postgres_database')

def get_postgres_connection():
    POSTGRES_USER = os.getenv("POSTGRES_USER", "postgres")
    POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "password")
    POSTGRES_DB = os.getenv("POSTGRES_DB", "product_catalog")
    POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")
    POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")
    
    log_step(logger, 1, f"Attempting to connect to PostgreSQL database: {POSTGRES_DB} on {POSTGRES_HOST}:{POSTGRES_PORT}")
    
    try:
        conn = psycopg2.connect(
            dbname=POSTGRES_DB,
            user=POSTGRES_USER,
            password=POSTGRES_PASSWORD,
            host=POSTGRES_HOST,
            port=POSTGRES_PORT,
            cursor_factory=RealDictCursor
        )
        log_step(logger, 2, "Successfully connected to PostgreSQL database")
        return conn
    except psycopg2.Error as e:
        log_step(logger, 3, f"Failed to connect to PostgreSQL database. Error: {str(e)}")
        raise

def close_postgres_connection(conn):
    if conn:
        log_step(logger, 4, "Closing PostgreSQL database connection")
        conn.close()
        log_step(logger, 5, "PostgreSQL database connection closed")

==================================================

backend/flink_jobs/debezium_sync.py :

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema
import psycopg2
import json
import os
from backend.app.core.logger import setup_logger, log_step

logger = setup_logger('debezium_sync')

# Kafka configuration for CDC
KAFKA_BROKER_URL = os.getenv('KAFKA_BROKER_URL', 'kafka:9092')
TOPIC_NAME = 'dbhistory.sqlite'

def update_postgresql(data):
    log_step(logger, 1, f"Updating PostgreSQL with operation: {data['op']}")
    pg_conn = psycopg2.connect(
        dbname=os.getenv("POSTGRES_DB", "product_catalog"),
        user=os.getenv("POSTGRES_USER", "postgres"),
        password=os.getenv("POSTGRES_PASSWORD", "password"),
        host=os.getenv("POSTGRES_HOST", "postgres"),
        port=os.getenv("POSTGRES_PORT", "5432")
    )
    cursor = pg_conn.cursor()

    try:
        if data['op'] == 'c':  # 'c' stands for create (insert)
            log_step(logger, 2, "Inserting new record into PostgreSQL")
            cursor.execute(
                "INSERT INTO products (name, description, price) VALUES (%s, %s, %s)",
                (data['after']['name'], data['after']['description'], data['after']['price'])
            )
        elif data['op'] == 'u':  # 'u' stands for update
            log_step(logger, 2, f"Updating record in PostgreSQL with id: {data['after']['id']}")
            cursor.execute(
                "UPDATE products SET name=%s, description=%s, price=%s WHERE id=%s",
                (data['after']['name'], data['after']['description'], data['after']['price'], data['after']['id'])
            )
        elif data['op'] == 'd':  # 'd' stands for delete
            log_step(logger, 2, f"Deleting record from PostgreSQL with id: {data['before']['id']}")
            cursor.execute(
                "DELETE FROM products WHERE id=%s", (data['before']['id'],)
            )

        pg_conn.commit()
        log_step(logger, 3, "PostgreSQL operation completed successfully")
    except Exception as e:
        log_step(logger, 3, f"Error updating PostgreSQL: {str(e)}")
        pg_conn.rollback()
    finally:
        cursor.close()
        pg_conn.close()
        log_step(logger, 4, "PostgreSQL connection closed")

def process_cdc_message(message):
    log_step(logger, 5, "Processing CDC message")
    try:
        data = json.loads(message)
        log_step(logger, 6, f"Parsed CDC message: {data}")
        update_postgresql(data)
        log_step(logger, 7, "CDC message processed successfully")
    except json.JSONDecodeError as e:
        log_step(logger, 7, f"Error decoding CDC message: {str(e)}")
    except Exception as e:
        log_step(logger, 7, f"Error processing CDC message: {str(e)}")

def cdc_sync_job():
    log_step(logger, 8, "Starting CDC sync job")
    env = StreamExecutionEnvironment.get_execution_environment()

    kafka_consumer = FlinkKafkaConsumer(
        topics=[TOPIC_NAME],
        value_deserializer=SimpleStringSchema(),
        properties={
            'bootstrap.servers': KAFKA_BROKER_URL,
            'group.id': 'flink_cdc_consumer'
        }
    )

    log_step(logger, 9, f"Created Kafka consumer for topic: {TOPIC_NAME}")

    kafka_stream = env.add_source(kafka_consumer)
    
    kafka_stream.map(process_cdc_message)
    
    log_step(logger, 10, "Executing Flink CDC Sync Job")
    env.execute("Flink CDC Sync Job")

if __name__ == '__main__':
    cdc_sync_job()

==================================================

backend/flink_jobs/flink_job.py :

import sys
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema
import sqlite3
import json
import requests
import os
from backend.app.core.logger import setup_logger, log_step

logger = setup_logger('flink_job')

# Kafka configuration
KAFKA_BROKER_URL = os.getenv('KAFKA_BROKER_URL', 'kafka:9092')
TOPIC_NAME = 'vendor_requests'

DEFAULT_IMAGE = 'image.png'

def get_vendor_api_url(vendor_id):
    try:
        log_step(logger, 1, f"Fetching API URL for vendor {vendor_id}")
        conn = sqlite3.connect('/databases/sqlite/products.db')
        cursor = conn.cursor()
        cursor.execute("SELECT api_url FROM vendors WHERE id = ?", (vendor_id,))
        result = cursor.fetchone()
        conn.close()
        if result:
            log_step(logger, 2, f"Found API URL for vendor {vendor_id}: {result[0]}")
            return result[0]
        else:
            log_step(logger, 2, f"No API URL found for vendor {vendor_id}")
            return None
    except Exception as e:
        log_step(logger, 2, f"Error fetching vendor API URL: {str(e)}")
        return None

def insert_into_sqlite(products, vendor_id):
    try:
        log_step(logger, 3, f"Inserting {len(products)} products into SQLite for vendor {vendor_id}")
        conn = sqlite3.connect('/databases/sqlite/products.db')
        cursor = conn.cursor()

        for product in products:
            cursor.execute(
                """
                INSERT OR REPLACE INTO products (id, name, description, price)
                VALUES (?, ?, ?, ?)
                """,
                (product['id'], product['title'], DEFAULT_IMAGE, product['price'])
            )
            cursor.execute(
                """
                INSERT OR REPLACE INTO products_vendors (product_id, vendor_id)
                VALUES (?, ?)
                """,
                (product['id'], vendor_id)
            )
        conn.commit()
        conn.close()
        log_step(logger, 4, f"Successfully inserted {len(products)} products into SQLite for vendor {vendor_id}")
    except Exception as e:
        log_step(logger, 4, f"Error inserting products into SQLite: {str(e)}")

def get_size(obj, seen=None):
    """Recursively calculate size of objects"""
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    seen.add(obj_id)
    if isinstance(obj, dict):
        size += sum([get_size(v, seen) for v in obj.values()])
        size += sum([get_size(k, seen) for k in obj.keys()])
    elif hasattr(obj, '__dict__'):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):
        size += sum([get_size(i, seen) for i in obj])
    return size

def process_message(message):
    try:
        log_step(logger, 1, f"Processing message. Message size: {len(message)} bytes")
        data = json.loads(message)
        log_step(logger, 2, f"Parsed data size: {get_size(data)} bytes")
        
        vendor_id = data.get('vendor_id')
        log_step(logger, 3, f"Processing data for vendor_id: {vendor_id}")

        api_url = get_vendor_api_url(vendor_id)
        if not api_url:
            log_step(logger, 4, f"Vendor with ID {vendor_id} not found or has no API URL")
            return

        log_step(logger, 5, f"Fetching data from API URL: {api_url}")
        response = requests.get(api_url)
        log_step(logger, 6, f"API response size: {len(response.content)} bytes")
        
        if response.status_code == 200:
            products = response.json().get('products', [])
            log_step(logger, 7, f"Fetched {len(products)} products from vendor {vendor_id}")
            log_step(logger, 8, f"Total size of products data: {get_size(products)} bytes")
            
            for i, product in enumerate(products[:5]):  # Log details of first 5 products
                log_step(logger, 9, f"Product {i+1} details: {json.dumps(product)[:200]}...")  # Log first 200 characters
            
            insert_into_sqlite(products, vendor_id)
        else:
            log_step(logger, 7, f"Failed to fetch data from vendor {vendor_id}, status code: {response.status_code}")
    except Exception as e:
        log_step(logger, 10, f"Exception occurred while processing message: {str(e)}")
        log_step(logger, 11, f"Full message content: {message[:1000]}...")  # Log first 1000 characters of the message

def flink_job():
    log_step(logger, 10, "Starting Flink job")
    env = StreamExecutionEnvironment.get_execution_environment()

    kafka_consumer = FlinkKafkaConsumer(
        topics=[TOPIC_NAME],
        deserialization_schema=SimpleStringSchema(),
        properties={
            'bootstrap.servers': KAFKA_BROKER_URL,
            'group.id': 'flink_consumer',
            'auto.offset.reset': 'earliest'
        }
    )

    log_step(logger, 11, f"Created Kafka consumer for topic: {TOPIC_NAME}")

    kafka_stream = env.add_source(kafka_consumer)
    kafka_stream.map(process_message)

    log_step(logger, 12, "Executing Flink Vendor Data Fetch Job")
    env.execute("Flink Vendor Data Fetch Job")

if __name__ == '__main__':
    flink_job()

==================================================

backend/flink_jobs/debezium_connectors/debezium-postgres-connector.json :

{
  "name": "postgres-connector",
  "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "postgres",
      "database.password": "password",
      "database.dbname": "product_catalog",
      "database.server.name": "postgres_server",
      "table.include.list": "public.products,public.vendors",
      "plugin.name": "pgoutput",
      "slot.name": "debezium_slot",
      "publication.name": "debezium_publication",
      "database.history.kafka.bootstrap.servers": "kafka:9092",
      "database.history.kafka.topic": "schema-changes.product_catalog",
      "transforms": "unwrap",
      "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "key.converter.schemas.enable": "false",
      "value.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter.schemas.enable": "false"
  }
}

==================================================

backend/flink_jobs/debezium_connectors/debezium-sqlite-connector.json :

{
  "name": "sqlite-connector",
  "config": {
      "connector.class": "io.debezium.connector.sqlite.SqliteConnector",
      "database.hostname": "fastapi",
      "database.port": "8000",
      "database.user": "debezium",
      "database.password": "dbz",
      "database.dbname": "/databases/sqlite/products.db",
      "database.server.name": "sqlite_server",
      "table.include.list": "products,vendors",
      "database.history.kafka.bootstrap.servers": "kafka:9092",
      "database.history.kafka.topic": "schema-changes.sqlite",
      "transforms": "unwrap",
      "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "key.converter.schemas.enable": "false",
      "value.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter.schemas.enable": "false"
  }
}

==================================================

databases/postgresql/init_postgres_schema.sql :

-- Vendors Tablosu
CREATE TABLE IF NOT EXISTS vendors (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL
);

-- Products Tablosu
CREATE TABLE IF NOT EXISTS products (
    id SERIAL PRIMARY KEY,
    vendor_id INTEGER NOT NULL,
    vendor_product_id INTEGER NOT NULL,
    name TEXT NOT NULL,
    description TEXT,
    price REAL NOT NULL,
    FOREIGN KEY (vendor_id) REFERENCES vendors(id),
    UNIQUE (vendor_id, vendor_product_id)
);


==================================================

databases/sqlite/init_schema.sql :

-- Create vendors table
CREATE TABLE IF NOT EXISTS vendors (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    site TEXT NOT NULL,
    api_url TEXT NOT NULL
);

-- Create products table
CREATE TABLE IF NOT EXISTS products (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    description TEXT,
    price REAL NOT NULL
);

-- Create products_vendors table for many-to-many relationship
CREATE TABLE IF NOT EXISTS products_vendors (
    product_id INTEGER,
    vendor_id INTEGER,
    PRIMARY KEY (product_id, vendor_id),
    FOREIGN KEY (product_id) REFERENCES products(id),
    FOREIGN KEY (vendor_id) REFERENCES vendors(id)
);


==================================================

databases/sqlite/products.db :

Binary file, content not displayed.


==================================================

databases/sqlite/seed_data.sql :

INSERT OR IGNORE INTO vendors (id, name, site, api_url) VALUES
(1, 'sample_vendor1', 'dummyjson.com', 'https://dummyjson.com/products?limit=10&skip=10&select=title,price');

INSERT OR IGNORE INTO vendors (id, name, site, api_url) VALUES
(2, 'sample_vendor2', 'dummyjson.com', 'https://dummyjson.com/products?limit=15&skip=15&select=title,price');


==================================================

flink/Dockerfile :

FROM flink:1.16.1

# Gerekli Python paketlerini yükleyin
RUN apt-get update && \
    apt-get install -y python3 python3-pip && \
    rm -rf /var/lib/apt/lists/*

RUN pip3 install --no-cache-dir requests psycopg2-binary

# İşlerinizi koyacağınız dizin
WORKDIR /flink_jobs

# Flink job dosyalarınızı kopyalayın
COPY ./backend/flink_jobs /flink_jobs

==================================================

mobile_app/assets/images/image.png :

Binary file, content not displayed.


==================================================

mobile_app/lib/main.dart :

import 'package:flutter/material.dart';
import 'screens/product_list_screen.dart';
import 'services/logger_service.dart';

void main() {
  WidgetsFlutterBinding.ensureInitialized();
  final logger = LoggerService();
  logger.info('Application started');
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key}); // Use 'super.key' directly in the constructor

  @override
  Widget build(BuildContext context) {
    final logger = LoggerService();
    logger.debug('Building MyApp');
    return MaterialApp(
      title: 'Unified Vendor Catalog',
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: const MyHomePage(title: 'Unified Vendor Catalog'),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage(
      {super.key,
      required this.title}); // Use 'super.key' directly in the constructor

  final String title;

  @override
  MyHomePageState createState() =>
      MyHomePageState(); // Make the state class public by removing the underscore
}

class MyHomePageState extends State<MyHomePage> {
  final logger = LoggerService();
  int _counter = 0;

  void _incrementCounter() {
    setState(() {
      _counter++;
    });
    logger.debug('Counter incremented to $_counter');
  }

  @override
  Widget build(BuildContext context) {
    logger.debug('Building MyHomePage');
    return Scaffold(
      appBar: AppBar(
        backgroundColor: Theme.of(context).colorScheme.inversePrimary,
        title: Text(widget.title),
      ),
      body: Center(
        child: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: <Widget>[
            const Text(
              'You have pushed the button this many times:',
            ),
            Text(
              '$_counter',
              style: Theme.of(context).textTheme.headlineMedium,
            ),
            ElevatedButton(
              onPressed: () {
                logger.info('Navigating to ProductListScreen');
                Navigator.push(
                  context,
                  MaterialPageRoute(
                      builder: (context) => const ProductListScreen()),
                );
              },
              child: const Text('Go to Product List'),
            ),
          ],
        ),
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: _incrementCounter,
        tooltip: 'Increment',
        child: const Icon(Icons.add),
      ),
    );
  }
}


==================================================

mobile_app/lib/screens/product_list_screen.dart :

import 'package:flutter/material.dart';
import 'package:http/http.dart' as http;
import 'dart:convert';
import '../services/logger_service.dart';

class ProductListScreen extends StatefulWidget {
  final int? vendorId;

  const ProductListScreen({super.key, this.vendorId});

  @override
  ProductListScreenState createState() => ProductListScreenState();
}

class ProductListScreenState extends State<ProductListScreen> {
  List products = [];
  bool isLoading = true;
  String message = '';
  final LoggerService logger = LoggerService();
  int retryCount = 0;
  static const int maxRetries = 3;

  @override
  void initState() {
    super.initState();
    logger.info('ProductListScreen initialized');
    fetchProducts();
  }

  Future<void> fetchProducts() async {
    String url = 'http://10.0.2.2:8000/products';
    if (widget.vendorId != null) {
      url += '?vendor_id=${widget.vendorId}';
    }
    logger.debug('Fetching products from URL: $url');

    try {
      final response = await http.get(Uri.parse(url));
      logger
          .debug('Received response with status code: ${response.statusCode}');
      logger.debug('Response body: ${response.body}');

      if (response.statusCode == 200) {
        final decodedBody = json.decode(response.body);
        setState(() {
          products = decodedBody['products'];
          isLoading = false;
          message = '';
        });
        logger.info('Successfully loaded ${products.length} products');
      } else if (response.statusCode == 202) {
        logger.info('Data is being fetched, waiting to retry');
        setState(() {
          message = 'Data is being fetched, please wait...';
        });
        if (retryCount < maxRetries) {
          retryCount++;
          logger.debug('Retry attempt $retryCount of $maxRetries');
          Future.delayed(const Duration(seconds: 5), fetchProducts);
        } else {
          setState(() {
            isLoading = false;
            message = 'Failed to load products after $maxRetries attempts';
          });
          logger.error('Max retry attempts reached');
        }
      } else {
        logger.error(
            'Failed to load products. Status code: ${response.statusCode}');
        setState(() {
          message =
              'Failed to load products. Status code: ${response.statusCode}';
          isLoading = false;
        });
      }
    } catch (e, stackTrace) {
      logger.error('Error fetching products', e, stackTrace);
      setState(() {
        message = 'Error: ${e.toString()}';
        isLoading = false;
      });
    }
  }

  @override
  Widget build(BuildContext context) {
    logger.debug('Building ProductListScreen. isLoading: $isLoading');
    return Scaffold(
      appBar: AppBar(
        title: const Text('Product List'),
        actions: [
          IconButton(
            icon: const Icon(Icons.refresh),
            onPressed: () {
              setState(() {
                isLoading = true;
                message = '';
                retryCount = 0;
              });
              fetchProducts();
            },
          ),
        ],
      ),
      body: isLoading
          ? const Center(child: CircularProgressIndicator())
          : message.isNotEmpty
              ? Center(child: Text(message))
              : products.isEmpty
                  ? const Center(child: Text('No products found'))
                  : ListView.builder(
                      itemCount: products.length,
                      itemBuilder: (context, index) {
                        final product = products[index];
                        logger.debug(
                            'Building list item for product: ${product['name']}');
                        return ListTile(
                          leading: const Image(
                            image: AssetImage('assets/images/image.png'),
                          ),
                          title: Text(product['name']),
                          subtitle: Text('\$${product['price']}'),
                        );
                      },
                    ),
    );
  }
}


==================================================

mobile_app/lib/services/logger_service.dart :

import 'dart:io' as io;
import 'dart:convert'; // Required for Encoding, utf8
import 'package:logger/logger.dart';
import 'package:path/path.dart' as path;

class LoggerService {
  static final LoggerService _instance = LoggerService._internal();
  late Logger logger;
  io.File? logFile;

  factory LoggerService() {
    return _instance;
  }

  LoggerService._internal() {
    _initLogger();
  }

  void _initLogger() {
    if (!_isWeb()) {
      final String logFilePath = _getLogFilePath();
      logFile = io.File(logFilePath);
    }

    logger = Logger(
      printer: PrettyPrinter(),
      output: _isWeb() ? null : FileOutput(file: logFile!),
    );
  }

  String _getLogFilePath() {
    final String projectRoot =
        _isWeb() ? '' : path.dirname(io.Platform.resolvedExecutable);
    final logsDir = io.Directory(path.join(projectRoot, 'logs'));
    if (!logsDir.existsSync()) {
      logsDir.createSync();
    }
    return path.join(logsDir.path, 'flutter_app.log');
  }

  bool _isWeb() {
    try {
      return identical(0, 0.0);
    } catch (e) {
      return true;
    }
  }

  void info(String message) {
    logger.i(message);
  }

  void debug(String message) {
    logger.d(message);
  }

  void error(String message, [dynamic error, StackTrace? stackTrace]) {
    final errorMessage = '$message Error: $error, StackTrace: $stackTrace';
    logger.e(errorMessage);
  }

  void warning(String message) {
    logger.w(message);
  }

  Future<String> getLogFilePath() async {
    return logFile?.path ?? '';
  }

  Future<String> getLogs() async {
    return logFile?.readAsString() ?? '';
  }

  Future<void> clearLogs() async {
    await logFile?.writeAsString('');
  }
}

class FileOutput extends LogOutput {
  final io.File file;
  final bool overrideExisting;
  final Encoding encoding;

  FileOutput({
    required this.file,
    this.overrideExisting = false,
    this.encoding = utf8,
  });

  @override
  void output(OutputEvent event) {
    final output = event.lines.join('\n');
    file.writeAsStringSync('$output\n',
        mode: io.FileMode.append, encoding: encoding);
  }
}


==================================================

