Project Structure:

multivendor-catalog/
    .gitattributes
    docker-compose.yml
    readme.md
    requirements.txt
    backend/
        app/
            Dockerfile
            main.py
            api/
                routes.py
            core/
                config.py
                database.py
                kafka_producer.py
                postgres_database.py
        flink_jobs/
            debezium_sync.py
            flink_job.py
            debezium_connectors/
                debezium-postgres-connector.json
                debezium-sqlite-connector.json
    databases/
        postgresql/
            init_postgres_schema.sql
        sqlite/
            init_schema.sql
            products.db
            seed_data.sql
    flink/
        Dockerfile
    mobile_app/
        assets/
            images/
                image.png
        lib/
            main.dart
            screens/
                product_list_screen.dart
            services/
    multivendor-catalog/
        .gitattributes

==================================================

File Contents:

.gitattributes :

# Auto detect text files and perform LF normalization
* text=auto


==================================================

docker-compose.yml :

services:
  fastapi:
    container_name: fastapi_service
    build:
      context: .
      dockerfile: backend/app/Dockerfile
    volumes:
      - ./backend/app:/app
      - ./databases/sqlite:/databases/sqlite
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=sqlite:///databases/sqlite/products.db
      - KAFKA_BROKER_URL=kafka:9092
      - POSTGRES_HOST=postgres
      - SQLITE_DB_PATH=/databases/sqlite/products.db
    depends_on:
      - kafka
      - postgres
      - debezium
    networks:
      - app_network

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka_service
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    depends_on:
      - zookeeper
    networks:
      - app_network

  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper_service
    ports:
      - "2181:2181"
    networks:
      - app_network

  postgres:
    image: postgres:13
    container_name: postgres_service
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: product_catalog
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./databases/postgresql/init_postgres_schema.sql:/docker-entrypoint-initdb.d/init_postgres_schema.sql
    ports:
      - "5432:5432"
    networks:
      - app_network

  debezium:
    image: debezium/connect:latest
    container_name: debezium_service
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
    ports:
      - "8083:8083"
    depends_on:
      - kafka
    networks:
      - app_network

  flink-jobmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink_jobmanager
    ports:
      - "8081:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    volumes:
      - ./backend/flink_jobs:/flink_jobs
      - ./databases/sqlite:/databases/sqlite
    networks:
      - app_network
    depends_on:
      - kafka
    entrypoint: >
      /bin/sh -c "
      /docker-entrypoint.sh jobmanager;
      flink run -d -py /flink_jobs/flink_job.py
      "

  flink-taskmanager:
    build:
      context: .
      dockerfile: flink/Dockerfile
    container_name: flink_taskmanager
    depends_on:
      - flink-jobmanager
    command: ["taskmanager"]
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    volumes:
      - ./databases/sqlite:/databases/sqlite
    networks:
      - app_network

volumes:
  postgres_data:

networks:
  app_network:
    driver: bridge


==================================================

readme.md :



==================================================

requirements.txt :

Binary file, content not displayed.


==================================================

backend/app/Dockerfile :

FROM python:3.9-slim

WORKDIR /app

# Sistem bağımlılıklarını yükleyin (sqlite3 dahil)
RUN apt-get update && apt-get install -y \
    gcc \
    libpq-dev \
    sqlite3 \
    && rm -rf /var/lib/apt/lists/*

# Copy the requirements file into the container
COPY ../../requirements.txt .

# Install the Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code
COPY ../../backend/app /app

# Ensure the module imports work correctly
ENV PYTHONPATH=/app

# Command to run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]


==================================================

backend/app/main.py :

# backend/app/main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from api.routes import router
from core.config import settings
import os
import sqlite3
import logging

# Define the lifespan handler
@asynccontextmanager
async def lifespan(app: FastAPI):
    logging.info("Starting up...")

    # Startup event
    db_path = '/databases/sqlite/products.db'
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    # Initialize schema
    with open('/databases/sqlite/init_schema.sql') as f:
        conn.executescript(f.read())

    # Check if vendors table is empty
    cursor.execute("SELECT COUNT(*) FROM vendors;")
    result = cursor.fetchone()
    vendor_count = result[0] if result else 0

    if vendor_count == 0:
        # Seed the vendors data
        seed_file_path = '/databases/sqlite/seed_data.sql'
        if os.path.exists(seed_file_path):
            with open(seed_file_path) as f:
                conn.executescript(f.read())

    conn.close()

    # Yield control to run the app
    logging.info("Startup complete.")
    yield
    logging.info("Shutting down...")

app = FastAPI(title=settings.PROJECT_NAME, lifespan=lifespan)

app.include_router(router)

origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


==================================================

backend/app/api/routes.py :

# backend/app/api/routes.py

from fastapi import APIRouter, Depends, HTTPException
from core.database import get_db_connection
from core.kafka_producer import send_data_to_kafka

router = APIRouter()

@router.get("/products")
def get_products(vendor_id: int = None, conn=Depends(get_db_connection)):
    cursor = conn.cursor()
    if vendor_id:
        # Belirli bir satıcı için ürünleri getir
        cursor.execute("""
            SELECT p.*
            FROM products p
            JOIN products_vendors pv ON p.id = pv.product_id
            WHERE pv.vendor_id = ?
        """, (vendor_id,))
    else:
        # Tüm ürünleri getir
        cursor.execute("SELECT * FROM products")
    products = cursor.fetchall()

    if not products:
        # Veri yok, veri çekme sürecini başlat
        if vendor_id:
            send_data_to_kafka({'vendor_id': vendor_id})
        else:
            # Tüm satıcılar için veri çekme talebi gönder
            cursor.execute("SELECT id FROM vendors")
            vendor_ids = cursor.fetchall()
            for vendor in vendor_ids:
                send_data_to_kafka({'vendor_id': vendor['id']})
        raise HTTPException(status_code=202, detail="Data is being fetched, please try again shortly.")
    
    return {"products": [dict(ix) for ix in products]}

@router.post("/request-vendor-data")
def request_vendor_data(vendor_id: int, conn=Depends(get_db_connection)):
    # Check if vendor exists
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM vendors WHERE id = ?", (vendor_id,))
    vendor = cursor.fetchone()
    if not vendor:
        raise HTTPException(status_code=400, detail="Invalid vendor_id")
    # Send message to Kafka
    send_data_to_kafka({'vendor_id': vendor_id})
    return {"message": f"Data fetch request for vendor {vendor_id} has been sent."}


==================================================

backend/app/core/config.py :

import os

class Settings:
    PROJECT_NAME: str = "Unified Vendor Catalog"
    SQLALCHEMY_DATABASE_URL: str = os.getenv("DATABASE_URL", "sqlite:///../../databases/sqlite/products.db")

settings = Settings()


==================================================

backend/app/core/database.py :

# backend/app/core/database.py

import sqlite3
from typing import Generator

def get_db_connection() -> Generator:
    db_path = '/databases/sqlite/products.db'
    conn = sqlite3.connect(db_path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
    finally:
        conn.close()


==================================================

backend/app/core/kafka_producer.py :

from kafka import KafkaProducer
import json
import os
import time

# Kafka configuration
KAFKA_BROKER_URL = os.getenv('KAFKA_BROKER_URL', 'kafka:9092')
TOPIC_NAME = 'vendor_requests'

producer = None

def get_kafka_producer():
    global producer
    if producer is None:
        for attempt in range(5):  # Retry up to 5 times
            try:
                producer = KafkaProducer(
                    bootstrap_servers=[KAFKA_BROKER_URL],
                    api_version=(0, 11, 5),
                    value_serializer=lambda v: json.dumps(v).encode('utf-8')
                )
                break
            except Exception as e:
                print(f"Attempt {attempt + 1}: Failed to connect to Kafka broker. Retrying in 5 seconds...")
                time.sleep(5)
        else:
            raise Exception("Could not establish connection to Kafka broker after multiple attempts.")
    return producer

def send_data_to_kafka(data):
    try:
        kafka_producer = get_kafka_producer()
        kafka_producer.send(TOPIC_NAME, value=data)
        kafka_producer.flush()
        print(f"Data sent to Kafka topic: {TOPIC_NAME}")
    except Exception as e:
        print(f"Failed to send data to Kafka: {e}")


==================================================

backend/app/core/postgres_database.py :

import os
import psycopg2
from psycopg2.extras import RealDictCursor

def get_postgres_connection():
    POSTGRES_USER = os.getenv("POSTGRES_USER", "postgres")
    POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "password")
    POSTGRES_DB = os.getenv("POSTGRES_DB", "product_catalog")
    POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")
    POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")
    
    conn = psycopg2.connect(
        dbname=POSTGRES_DB,
        user=POSTGRES_USER,
        password=POSTGRES_PASSWORD,
        host=POSTGRES_HOST,
        port=POSTGRES_PORT,
        cursor_factory=RealDictCursor
    )
    
    return conn


==================================================

backend/flink_jobs/debezium_sync.py :

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema
import psycopg2
import json

# Kafka configuration for CDC
KAFKA_BROKER_URL = 'localhost:9092'
TOPIC_NAME = 'dbhistory.sqlite'

def update_postgresql(data):
    pg_conn = psycopg2.connect(
        dbname="product_catalog",
        user="postgres",
        password="password",
        host="localhost",
        port="5432"
    )
    cursor = pg_conn.cursor()

    if data['op'] == 'c':  # 'c' stands for create (insert)
        cursor.execute(
            "INSERT INTO products (name, description, price) VALUES (%s, %s, %s)",
            (data['after']['name'], data['after']['description'], data['after']['price'])
        )
    elif data['op'] == 'u':  # 'u' stands for update
        cursor.execute(
            "UPDATE products SET name=%s, description=%s, price=%s WHERE id=%s",
            (data['after']['name'], data['after']['description'], data['after']['price'], data['after']['id'])
        )
    elif data['op'] == 'd':  # 'd' stands for delete
        cursor.execute(
            "DELETE FROM products WHERE id=%s", (data['before']['id'],)
        )

    pg_conn.commit()
    cursor.close()
    pg_conn.close()

def process_cdc_message(message):
    data = json.loads(message)
    update_postgresql(data)

def cdc_sync_job():
    env = StreamExecutionEnvironment.get_execution_environment()

    kafka_consumer = FlinkKafkaConsumer(
        topics=[TOPIC_NAME],
        value_deserializer=SimpleStringSchema(),
        properties={
            'bootstrap.servers': KAFKA_BROKER_URL,
            'group.id': 'flink_cdc_consumer'
        }
    )

    kafka_stream = env.add_source(kafka_consumer)
    
    kafka_stream.map(process_cdc_message)
    
    env.execute("Flink CDC Sync Job")

if __name__ == '__main__':
    cdc_sync_job()


==================================================

backend/flink_jobs/flink_job.py :

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema
import sqlite3
import json
import requests
import logging

# Kafka configuration
KAFKA_BROKER_URL = 'kafka:9092'  # Use 'kafka' as the service name within Docker
TOPIC_NAME = 'vendor_requests'

DEFAULT_IMAGE = 'image.png'
logging.basicConfig(level=logging.INFO)

def get_vendor_api_url(vendor_id):
    conn = sqlite3.connect('/databases/sqlite/products.db')
    cursor = conn.cursor()
    cursor.execute("SELECT api_url FROM vendors WHERE id = ?", (vendor_id,))
    result = cursor.fetchone()
    conn.close()
    if result:
        return result[0]
    else:
        return None

def insert_into_sqlite(products):
    try:
        conn = sqlite3.connect('/databases/sqlite/products.db')
        cursor = conn.cursor()

        for product in products:
            cursor.execute(
                """
                INSERT OR IGNORE INTO products (id, name, description, price)
                VALUES (?, ?, ?, ?)
                """,
                (product['id'], product['title'], DEFAULT_IMAGE, product['price'])
            )
        conn.commit()
        conn.close()
        logging.info(f"Inserted {len(products)} products into SQLite")
    except Exception as e:
        logging.exception(f"Exception occurred while inserting into SQLite: {e}")
    try:
        conn = sqlite3.connect('/databases/sqlite/products.db')
        cursor = conn.cursor()

        for product in products:
            cursor.execute(
                """
                INSERT OR IGNORE INTO products (id, name, description, price)
                VALUES (?, ?, ?, ?)
                """,
                (product['id'], product['title'], DEFAULT_IMAGE, product['price'])
            )
        conn.commit()
        conn.close()
        logging.info(f"Inserted {len(products)} products into SQLite")
    except Exception as e:
        logging.exception(f"Exception occurred while inserting into SQLite: {e}")
    conn = sqlite3.connect('/databases/sqlite/products.db')
    cursor = conn.cursor()

    for product in products:
        cursor.execute(
            """
            INSERT OR IGNORE INTO products (id, name, description, price)
            VALUES (?, ?, ?, ?)
            """,
            (product['id'], product['title'], DEFAULT_IMAGE, product['price'])
        )
    conn.commit()
    conn.close()

def insert_into_products_vendors(products, vendor_id):
    conn = sqlite3.connect('/databases/sqlite/products.db')
    cursor = conn.cursor()

    for product in products:
        cursor.execute(
            """
            INSERT OR IGNORE INTO products_vendors (product_id, vendor_id)
            VALUES (?, ?)
            """,
            (product['id'], vendor_id)
        )
    conn.commit()
    conn.close()

def process_message(message):
    try:
        logging.info(f"Received message: {message}")
        data = json.loads(message)
        vendor_id = data.get('vendor_id')

        api_url = get_vendor_api_url(vendor_id)
        if api_url:
            response = requests.get(api_url)
            if response.status_code == 200:
                products = response.json().get('products', [])
                logging.info(f"Fetched {len(products)} products from vendor {vendor_id}")
                insert_into_sqlite(products)
                insert_into_products_vendors(products, vendor_id)
                logging.info(f"Inserted products into SQLite for vendor {vendor_id}")
            else:
                logging.error(f"Failed to fetch data from vendor {vendor_id}, status code: {response.status_code}")
        else:
            logging.error(f"Vendor with ID {vendor_id} not found")
    except Exception as e:
        logging.exception(f"Exception occurred while processing message: {e}")
    data = json.loads(message)
    vendor_id = data.get('vendor_id')

    api_url = get_vendor_api_url(vendor_id)
    if api_url:
        response = requests.get(api_url)
        if response.status_code == 200:
            products = response.json().get('products', [])
            insert_into_sqlite(products)
            insert_into_products_vendors(products, vendor_id)
            print(f"Fetched and inserted products from vendor {vendor_id}")
        else:
            print(f"Failed to fetch data from vendor {vendor_id}")
    else:
        print(f"Vendor with ID {vendor_id} not found")

def flink_job():
    env = StreamExecutionEnvironment.get_execution_environment()

    kafka_consumer = FlinkKafkaConsumer(
        topics=[TOPIC_NAME],
        deserialization_schema=SimpleStringSchema(),
        properties={
            'bootstrap.servers': KAFKA_BROKER_URL,
            'group.id': 'flink_consumer'
        }
    )

    kafka_stream = env.add_source(kafka_consumer)

    # Process each message from Kafka
    kafka_stream.map(process_message)

    env.execute("Flink Vendor Data Fetch Job")

if __name__ == '__main__':
    flink_job()


==================================================

backend/flink_jobs/debezium_connectors/debezium-postgres-connector.json :

{
    "name": "postgres-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "postgres",
      "database.password": "password",
      "database.dbname": "product_catalog",
      "database.server.name": "postgres_server",
      "plugin.name": "pgoutput",
      "slot.name": "debezium_slot",
      "publication.name": "debezium_publication",
      "database.history.kafka.bootstrap.servers": "kafka:9092",
      "database.history.kafka.topic": "dbhistory.postgres"
    }
  }
  

==================================================

backend/flink_jobs/debezium_connectors/debezium-sqlite-connector.json :

{
    "name": "sqlite-connector",
    "config": {
      "connector.class": "io.debezium.connector.sqlite.SqliteConnector",
      "database.filename": "/var/lib/sqlite/products.db",
      "database.server.name": "sqlite_server",
      "database.history.kafka.bootstrap.servers": "kafka:9092",
      "database.history.kafka.topic": "dbhistory.sqlite"
    }
  }
  

==================================================

databases/postgresql/init_postgres_schema.sql :

-- Vendors Tablosu
CREATE TABLE IF NOT EXISTS vendors (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL
);

-- Products Tablosu
CREATE TABLE IF NOT EXISTS products (
    id SERIAL PRIMARY KEY,
    vendor_id INTEGER NOT NULL,
    vendor_product_id INTEGER NOT NULL,
    name TEXT NOT NULL,
    description TEXT,
    price REAL NOT NULL,
    FOREIGN KEY (vendor_id) REFERENCES vendors(id),
    UNIQUE (vendor_id, vendor_product_id)
);


==================================================

databases/sqlite/init_schema.sql :

-- Create vendors table
CREATE TABLE IF NOT EXISTS vendors (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    site TEXT NOT NULL,
    api_url TEXT NOT NULL
);

-- Create products table
CREATE TABLE IF NOT EXISTS products (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    description TEXT,
    price REAL NOT NULL
);

-- Create products_vendors table for many-to-many relationship
CREATE TABLE IF NOT EXISTS products_vendors (
    product_id INTEGER,
    vendor_id INTEGER,
    PRIMARY KEY (product_id, vendor_id),
    FOREIGN KEY (product_id) REFERENCES products(id),
    FOREIGN KEY (vendor_id) REFERENCES vendors(id)
);


==================================================

databases/sqlite/products.db :

Binary file, content not displayed.


==================================================

databases/sqlite/seed_data.sql :

INSERT OR IGNORE INTO vendors (id, name, site, api_url) VALUES
(1, 'sample_vendor1', 'dummyjson.com', 'https://dummyjson.com/products?limit=10&skip=10&select=title,price');

INSERT OR IGNORE INTO vendors (id, name, site, api_url) VALUES
(2, 'sample_vendor2', 'dummyjson.com', 'https://dummyjson.com/products?limit=15&skip=15&select=title,price');


==================================================

flink/Dockerfile :

FROM flink:1.16.1

# Gerekli Python paketlerini yükleyin
RUN apt-get update && \
    apt-get install -y python3 python3-pip && \
    rm -rf /var/lib/apt/lists/*

RUN pip3 install --no-cache-dir requests psycopg2-binary

# İşlerinizi koyacağınız dizin
WORKDIR /flink_jobs

# Flink job dosyalarınızı kopyalayın
COPY ./backend/flink_jobs /flink_jobs

==================================================

mobile_app/assets/images/image.png :

Binary file, content not displayed.


==================================================

mobile_app/lib/main.dart :

import 'package:flutter/material.dart';
import 'screens/product_list_screen.dart';

void main() {
  runApp(MyApp());
}

class MyApp extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'Unified Vendor Catalog',
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: ProductListScreen(),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage({super.key, required this.title});

  // This widget is the home page of your application. It is stateful, meaning
  // that it has a State object (defined below) that contains fields that affect
  // how it looks.

  // This class is the configuration for the state. It holds the values (in this
  // case the title) provided by the parent (in this case the App widget) and
  // used by the build method of the State. Fields in a Widget subclass are
  // always marked "final".

  final String title;

  @override
  State<MyHomePage> createState() => _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {
  int _counter = 0;

  void _incrementCounter() {
    setState(() {
      // This call to setState tells the Flutter framework that something has
      // changed in this State, which causes it to rerun the build method below
      // so that the display can reflect the updated values. If we changed
      // _counter without calling setState(), then the build method would not be
      // called again, and so nothing would appear to happen.
      _counter++;
    });
  }

  @override
  Widget build(BuildContext context) {
    // This method is rerun every time setState is called, for instance as done
    // by the _incrementCounter method above.
    //
    // The Flutter framework has been optimized to make rerunning build methods
    // fast, so that you can just rebuild anything that needs updating rather
    // than having to individually change instances of widgets.
    return Scaffold(
      appBar: AppBar(
        // TRY THIS: Try changing the color here to a specific color (to
        // Colors.amber, perhaps?) and trigger a hot reload to see the AppBar
        // change color while the other colors stay the same.
        backgroundColor: Theme.of(context).colorScheme.inversePrimary,
        // Here we take the value from the MyHomePage object that was created by
        // the App.build method, and use it to set our appbar title.
        title: Text(widget.title),
      ),
      body: Center(
        // Center is a layout widget. It takes a single child and positions it
        // in the middle of the parent.
        child: Column(
          // Column is also a layout widget. It takes a list of children and
          // arranges them vertically. By default, it sizes itself to fit its
          // children horizontally, and tries to be as tall as its parent.
          //
          // Column has various properties to control how it sizes itself and
          // how it positions its children. Here we use mainAxisAlignment to
          // center the children vertically; the main axis here is the vertical
          // axis because Columns are vertical (the cross axis would be
          // horizontal).
          //
          // TRY THIS: Invoke "debug painting" (choose the "Toggle Debug Paint"
          // action in the IDE, or press "p" in the console), to see the
          // wireframe for each widget.
          mainAxisAlignment: MainAxisAlignment.center,
          children: <Widget>[
            const Text(
              'You have pushed the button this many times:',
            ),
            Text(
              '$_counter',
              style: Theme.of(context).textTheme.headlineMedium,
            ),
          ],
        ),
      ),
      floatingActionButton: FloatingActionButton(
        onPressed: _incrementCounter,
        tooltip: 'Increment',
        child: const Icon(Icons.add),
      ), // This trailing comma makes auto-formatting nicer for build methods.
    );
  }
}


==================================================

mobile_app/lib/screens/product_list_screen.dart :

import 'package:flutter/material.dart';
import 'package:http/http.dart' as http;
import 'dart:convert';

class ProductListScreen extends StatefulWidget {
  final int? vendorId;

  ProductListScreen({this.vendorId});

  @override
  _ProductListScreenState createState() => _ProductListScreenState();
}

class _ProductListScreenState extends State<ProductListScreen> {
  List products = [];
  bool isLoading = true;
  String message = '';

  @override
  void initState() {
    super.initState();
    fetchProducts();
  }

  void fetchProducts() async {
    String url = 'http://10.0.2.2:8000/products';
    if (widget.vendorId != null) {
      url += '?vendor_id=${widget.vendorId}';
    }
    final response = await http.get(Uri.parse(url));

    if (response.statusCode == 200) {
      setState(() {
        products = json.decode(response.body)['products'];
        isLoading = false;
      });
    } else if (response.statusCode == 202) {
      // Veri çekiliyor, biraz bekleyip tekrar dene
      setState(() {
        message = 'Data is being fetched, please wait...';
      });
      Future.delayed(Duration(seconds: 5), fetchProducts);
    } else {
      setState(() {
        message = 'Failed to load products';
        isLoading = false;
      });
    }
  }

  @override
  Widget build(BuildContext context) {
    if (isLoading) {
      return Scaffold(
        appBar: AppBar(
          title: Text('Product List'),
        ),
        body: Center(
          child: message.isEmpty ? CircularProgressIndicator() : Text(message),
        ),
      );
    } else {
      return Scaffold(
        appBar: AppBar(
          title: Text('Product List'),
        ),
        body: ListView.builder(
          itemCount: products.length,
          itemBuilder: (context, index) {
            final product = products[index];
            return ListTile(
              leading:
                  Image.asset('assets/images/image.png'), // Varsayılan resim
              title: Text(product['name']),
              subtitle: Text('\$${product['price']}'),
            );
          },
        ),
      );
    }
  }
}


==================================================

multivendor-catalog/.gitattributes :

# Auto detect text files and perform LF normalization
* text=auto


==================================================

